from langchain_core.outputs import LLMResult
from operations.settings import settings

from langchain_core.callbacks.base import BaseCallbackHandler
from langchain_core.runnables.base import Runnable


class CostTrackingCallback(BaseCallbackHandler):
    """ä½¿ç”¨é›†ä¸­é…ç½®çš„æˆæœ¬è¿½è¸ªå›è°ƒ"""
    def __init__(self):
        super().__init__()
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_cost_usd = 0.0
        self.call_records = []

    def _detect_model_name(self, ai_message) -> str:    # noqa
        """ä»AIMessageä¸­æ£€æµ‹æ¨¡å‹åç§°ï¼ˆæ ¹æ®ä½ çš„æä¾›å•†è°ƒæ•´ï¼‰"""
        # ç¤ºä¾‹ï¼šä» response_metadata æå–
        metadata = getattr(ai_message, 'response_metadata', {})
        # ä¸åŒæä¾›å•†å­—æ®µå¯èƒ½ä¸åŒï¼Œéœ€è¦é€‚é…
        name = metadata.get('model_name') or metadata.get('model') or 'unknown'
        # å¯ä»¥åšä¸€ä¸ªæ˜ å°„ï¼Œå°†æä¾›å•†åç§°æ ‡å‡†åŒ–ä¸ºä½ é…ç½®è¡¨ä¸­çš„é”®
        return name.lower()

    def on_tool_start(self, serialized, input_str, **kwargs):
        print(f"[å›è°ƒ] å·¥å…·è°ƒç”¨å¼€å§‹: {serialized.get('name', 'unknown')}")

    def on_tool_end(self, output, **kwargs):
        print(f"[å›è°ƒ] å·¥å…·è°ƒç”¨ç»“æŸï¼Œè¿”å›: {str(output)[:50]}...")

    def on_llm_end(self, response: LLMResult, **kwargs):
        print(f"[LLMå¤„ç†å®Œæˆ]")
        print(response)
        print("------")
        print(type(response))

        try:
            if response.generations:
                first_gen = response.generations[0][0]

                if hasattr(first_gen, 'message'):
                    ai_message = first_gen.message

                    if hasattr(ai_message, 'usage_metadata'):
                        tokens = ai_message.usage_metadata
                        input_tokens = tokens.get('input_tokens', 0)
                        output_tokens = tokens.get('output_tokens', 0)

                        # ä»é…ç½®ç³»ç»Ÿè·å–ä»·æ ¼å¹¶è®¡ç®—æˆæœ¬
                        model_name = self._detect_model_name(ai_message)    # éœ€è¦å®ç°ä¸€ä¸ªæ¨¡å‹åæ£€æµ‹æ–¹æ³•
                        pricing = settings.get_pricing(model_name)  # ä½¿ç”¨å…¨å±€é…ç½®

                        # è®¡ç®—æœ¬æ¬¡è°ƒç”¨æˆæœ¬
                        call_cost = (
                            (input_tokens / 1000) * pricing.input_price_per_1k +
                            (output_tokens / 1000) * pricing.output_price_per_1k
                        )

                        # ç´¯åŠ Tokenå’Œå•ä»·åˆ°å®ä¾‹å˜é‡
                        self.total_input_tokens += input_tokens
                        self.total_output_tokens += output_tokens
                        self.total_cost_usd += call_cost
                        print(f"ğŸ’° å•æ¬¡æˆæœ¬: ${call_cost:.6f} | ç´¯è®¡æˆæœ¬: ${self.total_cost_usd:.6f}")

                        # è®°å½•æœ¬æ¬¡è°ƒç”¨æ˜ç»†ï¼ˆå¯é€‰ï¼‰
                        self.call_records.append({
                            'run_id': kwargs.get('run_id'),
                            'input_tokens': input_tokens,
                            'output_tokens': output_tokens,
                            'total_tokens': input_tokens + output_tokens,
                            'content_preview': ai_message.content[:50]
                        })

                        print(f"âœ… å•æ¬¡Tokenæ•°æ® -> è¾“å…¥: {input_tokens}, è¾“å‡º: {output_tokens}")
                        print(f"ğŸ“Š ç´¯è®¡Tokenæ•°æ® -> è¾“å…¥: {self.total_input_tokens}, è¾“å‡º: {self.total_output_tokens}")

        except Exception as e:
            print(f"è§£æå“åº”æ—¶å‡ºé”™: {e}")

    def get_summary(self):
        """è·å–ç´¯è®¡æ‘˜è¦"""
        total_tokens = self.total_input_tokens + self.total_output_tokens
        return {
            'total_input_tokens': self.total_input_tokens,
            'total_output_tokens': self.total_output_tokens,
            'total_tokens': total_tokens,
            'call_count': len(self.call_records)
        }
